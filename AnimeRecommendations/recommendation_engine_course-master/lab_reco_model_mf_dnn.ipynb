{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YiHsien/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, sys, numpy as np, pandas as pd, tensorflow as tf, re, codecs, json, time\n",
    "import pickle, collections, random, math, numbers, scipy.sparse as sp, itertools, shutil\n",
    "\n",
    "def reload(mName):\n",
    "    import importlib\n",
    "    if mName in sys.modules:\n",
    "        del sys.modules[mName]\n",
    "    return importlib.import_module(mName)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict, Counter\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "utils = reload('utils.utils')\n",
    "from utils.utils import *\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=100)\n",
    "randomSeed = 88\n",
    "np.random.seed(randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## Data Prepare\n",
    "- mapping user id, movie id to zero start indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, movies, uidEnc, midEnc, nUsers, nMovies, midMap, tr, te, trRatingMat, teRatingMat = prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以leave one out方式產生 train data, test data\n",
    "1. 每一筆資料有兩部分: [user query] + [item meta]\n",
    "2. movie多加了兩個欄位: avg_rating, year\n",
    "    - avg_rating是所有user對於同一部電影的平均評分\n",
    "    - year則是從title extract出來的資訊\n",
    "3. 每一筆user query 包含所有user movie history, 除了當前的rating movie (candidate movie)\n",
    "4. test data的user query來自於train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69399, 4) (30605, 4) (100004, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tr.shape,te.shape,ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[5, 13, 10, 1, 9]</td>\n",
       "      <td>0.749438</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[5, 10, 9]</td>\n",
       "      <td>0.644860</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>0.591337</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>[1, 0, 3]</td>\n",
       "      <td>0.418803</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title             genres  avg_rating  \\\n",
       "0        0                    Toy Story (1995)  [5, 13, 10, 1, 9]    0.749438   \n",
       "1        1                      Jumanji (1995)         [5, 10, 9]    0.644860   \n",
       "2        2             Grumpier Old Men (1995)             [1, 3]    0.591337   \n",
       "3        3            Waiting to Exhale (1995)          [1, 0, 3]    0.418803   \n",
       "4        4  Father of the Bride Part II (1995)                [1]    0.615079   \n",
       "\n",
       "       year  \n",
       "0  0.815789  \n",
       "1  0.815789  \n",
       "2  0.815789  \n",
       "3  0.815789  \n",
       "4  0.815789  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>query_movie_ids</th>\n",
       "      <th>genres</th>\n",
       "      <th>avg_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>candidate_movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.835749</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>931</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1083, 833, 859, 30, 1111, 906, 1017, 104...</td>\n",
       "      <td>[4, 6, 2]</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1515</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 833, 859, 30, 1111, 906, 1017, 104...</td>\n",
       "      <td>[9, 7, 3, 2]</td>\n",
       "      <td>0.621795</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1083</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 1083, 859, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[13, 10, 0, 14]</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>833</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[931, 1515, 1083, 833, 30, 1111, 906, 1017, 10...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>859</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                    query_movie_ids  \\\n",
       "0        0  [1515, 1083, 833, 859, 30, 1111, 906, 1017, 10...   \n",
       "1        0  [931, 1083, 833, 859, 30, 1111, 906, 1017, 104...   \n",
       "2        0  [931, 1515, 833, 859, 30, 1111, 906, 1017, 104...   \n",
       "3        0  [931, 1515, 1083, 859, 30, 1111, 906, 1017, 10...   \n",
       "4        0  [931, 1515, 1083, 833, 30, 1111, 906, 1017, 10...   \n",
       "\n",
       "            genres  avg_rating      year  candidate_movie_id  rating  \n",
       "0              [0]    0.835749  0.763158                 931     4.0  \n",
       "1        [4, 6, 2]    0.782609  0.605263                1515     4.0  \n",
       "2     [9, 7, 3, 2]    0.621795  0.789474                1083     3.5  \n",
       "3  [13, 10, 0, 14]    0.711640  0.342105                 833     3.0  \n",
       "4              [2]    0.676768  0.824561                 859     3.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_trans, genres_enc = utils.doMovies(movies)\n",
    "movie_trans[\"avg_rating\"] = ratings.groupby(\"movieId\").rating.mean()\n",
    "movie_trans[\"avg_rating\"] = minmax_scale(movie_trans.avg_rating.fillna(ratings.rating.mean()))\n",
    "movie_trans[\"year\"] = movie_trans.title.str.findall(\"\\(\\s*(\\d+)\\s*\\)\").map(lambda lst: int(lst[-1]) if len(lst) else None)\n",
    "movie_trans[\"year\"] = minmax_scale(movie_trans.year.fillna(movie_trans.year.median()))\n",
    "n_genres = len(genres_enc.enc)\n",
    "\n",
    "trProcessed = loo_preprocess(tr, movie_trans)\n",
    "teProcessed = loo_preprocess(te, movie_trans, tr, is_train=False)\n",
    "trProcessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69399, 7)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trProcessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teProcessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Function\n",
    "1. 由於tensorflow placeholder不支援變動長度的columns, 需透過padding zero(補零)帶入\n",
    "2. 每個變動長度的column, 需要再給lens描述每一筆資料的長度, ex: genres, genres_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dataFn(data, n_batch=128, shuffle=False):\n",
    "    def fn():\n",
    "        dataInner = data.copy()\n",
    "        indices = get_minibatches_idx(len(dataInner), n_batch, shuffle=shuffle)\n",
    "        for ind in indices:\n",
    "            yield do_multi(dataInner.iloc[ind], [\"query_movie_ids\", \"genres\"])\n",
    "    return fn\n",
    "\n",
    "for i, e in enumerate(dataFn(trProcessed, n_batch=5, shuffle=True)(), 1):\n",
    "    # print(e)\n",
    "    break\n",
    "pd.DataFrame(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MF + DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMfDNN(object):\n",
    "    def __init__(self, n_items, n_genres, dim=32, learning_rate=0.01, reg=0.05, modelDir=\"./model/model_mf_with_history\"):\n",
    "        self.n_items = n_items\n",
    "        self.n_genres = n_genres\n",
    "        self.dim = dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "        self.modelDir = modelDir\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # inputs parts\n",
    "            graph_inputs(self)\n",
    "            # embedding parts\n",
    "            graph_embedding(self)\n",
    "            # deep neural network\n",
    "            graph_dnn(self)\n",
    "            # computation parts\n",
    "            graph_computation(self)\n",
    "            # loss parts\n",
    "            graph_loss(self)\n",
    "            self.saver = tf.train.Saver(tf.global_variables())\n",
    "            self.graph = graph\n",
    "        \n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        return fit(self, sess, trainGen, testGen, reset=reset, nEpoch=nEpoch)\n",
    "    \n",
    "    def predict(self, sess, user_queries, items):\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return sess.run(self.pred, feed_dict={\n",
    "            self.isTrain: False,\n",
    "            self.user_id: user_queries[\"user_id\"],\n",
    "            self.query_movie_ids: user_queries[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: user_queries[\"query_movie_ids_len\"],\n",
    "\n",
    "            self.genres: items[\"genres\"],\n",
    "            self.genres_len: items[\"genres_len\"],\n",
    "            self.avg_rating: items[\"avg_rating\"],\n",
    "            self.year: items[\"year\"],\n",
    "            self.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "    \n",
    "    def resetModel(self, modelDir):\n",
    "        \"\"\"刪除model dir\"\"\"\n",
    "        shutil.rmtree(path=modelDir, ignore_errors=True)\n",
    "        os.makedirs(modelDir)\n",
    "        \n",
    "    def ckpt(self, sess, modelDir):\n",
    "        \"\"\"load latest saved model\"\"\"\n",
    "        latestCkpt = tf.train.latest_checkpoint(modelDir)\n",
    "        if latestCkpt:\n",
    "            self.saver.restore(sess, latestCkpt)\n",
    "        return latestCkpt\n",
    "    \n",
    "    def feed_dict(self, data, mode=\"train\"):\n",
    "        ret = {\n",
    "            self.user_id: data[\"user_id\"],\n",
    "            self.query_movie_ids: data[\"query_movie_ids\"],\n",
    "            self.query_movie_ids_len: data[\"query_movie_ids_len\"],\n",
    "            self.genres: data[\"genres\"],\n",
    "            self.genres_len: data[\"genres_len\"],\n",
    "            self.avg_rating: data[\"avg_rating\"],\n",
    "            self.year: data[\"year\"],\n",
    "            self.candidate_movie_id: data[\"candidate_movie_id\"]\n",
    "        }\n",
    "        ret[self.isTrain] = False\n",
    "        if mode != \"infer\":\n",
    "            ret[self.rating] = data[\"rating\"]\n",
    "            if mode == \"train\":\n",
    "                ret[self.isTrain] = True\n",
    "            elif mode == \"eval\":\n",
    "                pass\n",
    "        return ret\n",
    "\n",
    "    def epochLoss(self, sess, dataGen, tpe=\"rmse\"):\n",
    "        totLoss, totCnt = 0, 0\n",
    "        for data in dataGen():\n",
    "            lossTensor = self.rmse_loss if tpe == \"rmse\" else self.mae_loss\n",
    "            loss = sess.run(lossTensor, feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            totLoss += (loss ** 2 if tpe == \"rmse\" else loss) * len(data[\"query_movie_ids\"])\n",
    "            totCnt += len(data[\"query_movie_ids\"])\n",
    "        return np.sqrt(totLoss / totCnt) if tpe == \"rmse\" else totLoss / totCnt\n",
    "    \n",
    "    def evaluateRMSE(self, sess, dataGen):\n",
    "        \"\"\"計算root mean square error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"rmse\")\n",
    "\n",
    "    def evaluateMAE(self, sess, dataGen):\n",
    "        \"\"\"計算 mean absolutely error\"\"\"\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "        return self.epochLoss(sess, dataGen, tpe=\"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定Inputs placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_inputs(self):\n",
    "    # inputs/id_user:0\n",
    "    with tf.variable_scope(\"inputs\"):\n",
    "        self.isTrain = tf.placeholder(tf.bool, None)\n",
    "        # user data\n",
    "        self.user_id = tf.placeholder(tf.int32, [None])\n",
    "        self.query_movie_ids = tf.placeholder(tf.int32, [None, None])\n",
    "        self.query_movie_ids_len = tf.placeholder(tf.int32, [None])\n",
    "        # item data\n",
    "        self.genres = tf.placeholder(tf.int32, [None, None])\n",
    "        self.genres_len = tf.placeholder(tf.int32, [None])\n",
    "        self.avg_rating = tf.placeholder(tf.float32, [None])\n",
    "        self.year = tf.placeholder(tf.float32, [None])\n",
    "        self.candidate_movie_id = tf.placeholder(tf.int32, [None])\n",
    "        self.rating = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding部分 = One Hot Encoding + Linear Transformation<br/>\n",
    "### tensorflow 提供 tf.nn.embedding_lookup function\n",
    "1. user and item 都使用embedding\n",
    "2. **user bias, item bias 個別使用query_emb, item_repr projection取得, projection方式即使用tensorflow tf.matmul運算即可**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_embedding(self):\n",
    "    init_fn = tf.glorot_normal_initializer()\n",
    "    emb_init_fn = tf.glorot_uniform_initializer()\n",
    "    self.b_global = tf.Variable(emb_init_fn(shape=[]), name=\"b_global\")\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        self.w_query_movie_ids = tf.Variable(emb_init_fn(shape=[self.n_items, dim]), name=\"w_query_movie_ids\")\n",
    "        self.b_query_movie_ids = tf.Variable(emb_init_fn(shape=[dim]), name=\"b_query_movie_ids\")\n",
    "        self.w_candidate_movie_id = tf.Variable(init_fn(shape=[self.n_items, dim]), name=\"w_candidate_movie_id\")\n",
    "        self.b_candidate_movie_id = tf.Variable(init_fn(shape=[dim + 8 + 2]), name=\"b_candidate_movie_id\")\n",
    "        self.w_genres = tf.Variable(emb_init_fn(shape=[self.n_genres, 8]), name=\"w_genres\")\n",
    "\n",
    "        # query_movie embedding\n",
    "        '''sqrtn aggregation(pooling), X: data, W: weight\n",
    "               X_1*W_1 + X_2*W_2 + ... + X_n*W_n / sqrt(W_1**2 + W_2**2 + ... W_n**2)\n",
    "             = weighted sum of X and normalized W\n",
    "           here data = self.query_emb, weight = query_movie_mask '''\n",
    "        self.query_emb = tf.nn.embedding_lookup(self.w_query_movie_ids, self.query_movie_ids)\n",
    "        query_movie_mask = tf.expand_dims(tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.query_movie_ids_len)), 1), -1)\n",
    "        self.query_emb = tf.reduce_sum(self.query_emb * query_movie_mask, 1)\n",
    "        self.query_bias = tf.matmul(self.query_emb, self.b_query_movie_ids[:, tf.newaxis])\n",
    "\n",
    "        # candidate_movie embedding\n",
    "        self.candidate_emb = tf.nn.embedding_lookup(self.w_candidate_movie_id, self.candidate_movie_id)\n",
    "\n",
    "        # genres embedding\n",
    "        '''sqrtn aggregation(pooling), X: data, W: weight\n",
    "               X_1*W_1 + X_2*W_2 + ... + X_n*W_n / sqrt(W_1**2 + W_2**2 + ... W_n**2)\n",
    "             = weighted sum of X and normalized W\n",
    "           here data = self.genres_emb, weight = genres_mask '''\n",
    "        self.genres_emb = tf.nn.embedding_lookup(self.w_genres, tf.to_int32(self.genres))\n",
    "        genres_mask = tf.expand_dims( tf.nn.l2_normalize(tf.to_float(tf.sequence_mask(self.genres_len)), 1), -1)\n",
    "        self.genres_emb = tf.reduce_sum(self.genres_emb * genres_mask, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN: Item(Movie Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_dnn(self):\n",
    "    with tf.variable_scope(\"dnn\"):\n",
    "        init_fn = tf.glorot_uniform_initializer()\n",
    "        # encode [item embedding + item metadata]\n",
    "        self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "        self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "        \n",
    "        # Do: 目前兩層DNN, 可以試著增加或減少hidden layer, 但是最後要跟query_emb內積, 最後一層維度必須 = dim\n",
    "        # Example:\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, 64, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, 32, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, self.dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "\n",
    "        # Do: 若有overfitting後可嘗試dropout\n",
    "        # dp_scale = 0.5\n",
    "        self.item_repr = tf.layers.dense(self.item_repr, self.dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "        # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "        self.item_repr = tf.layers.dense(self.item_repr, self.dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "        # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation: 帶公式\n",
    "$$ u_i \\cdot m_j + b_u + b_m + b_{global} $$\n",
    "\n",
    "1. self.infer與self.pred節點是帶同樣的公式, 但是self.pred節點快上許多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_computation(self):\n",
    "    with tf.variable_scope(\"computation\"):\n",
    "        infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "        infer = tf.add(infer, self.b_global)\n",
    "        infer = tf.add(infer, self.query_bias)\n",
    "        self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "        # one query for all items\n",
    "        self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                    tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "1. #### **這裡沒有使用 regularization term, (optinal! 也可自己補上)**\n",
    "2. 使用MSE優化整個model\n",
    "3. RMSE and MAE loss節點並不在training graph中, 主要在eval的時候可以觀察變化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_loss(self):\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        self.loss = tf.losses.mean_squared_error(labels=self.rating[:, tf.newaxis], predictions=self.infer)\n",
    "        # for eval\n",
    "        self.rmse_loss = tf.sqrt(self.loss)\n",
    "        self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.rating[:, tf.newaxis]))\n",
    "\n",
    "    with tf.variable_scope(\"train\"):\n",
    "        # Do: 嘗試不同的Optimizer\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True).minimize(self.loss)\n",
    "        # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit: Training Function\n",
    "1. reset: if True, clean the checkpoints save data\n",
    "2. 計算epoch loss: 每個batch的loss * batch數量總和後除以epoch總數\n",
    "3. training過程中會記錄valid loss, 只會儲存最低的loss => 另一種對付overfitting的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if reset:\n",
    "        print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "        self.resetModel(self.modelDir)\n",
    "    # try: 試著重上次儲存的model再次training\n",
    "    self.ckpt(sess, self.modelDir)\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    minLoss = 1e7\n",
    "    for ep in range(1, nEpoch + 1):\n",
    "        tr_loss, tr_total = 0, 0\n",
    "        for i, data in enumerate(trainGen(), 1):\n",
    "            loss, _ = sess.run([self.rmse_loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "            tr_loss += loss ** 2 * len(data[\"query_movie_ids\"])\n",
    "            tr_total += len(data[\"query_movie_ids\"])\n",
    "            print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "        if testGen is not None:\n",
    "            epochLoss = self.epochLoss(sess, testGen)\n",
    "\n",
    "        tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "        if minLoss > epochLoss:\n",
    "            tpl += \", saving ...\"\n",
    "            self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "            minLoss = epochLoss\n",
    "\n",
    "        end = time.time()\n",
    "        print(tpl % (ep, np.sqrt(tr_loss / tr_total), epochLoss, end - start))\n",
    "        start = end\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "n_batch = 128\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.0001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 3\n",
    "# 非必要: 改動model dir\n",
    "modelDir = \"./model/model_mf_with_dnn\"\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "model = ModelMfDNN(\n",
    "            n_items=nMovies,\n",
    "            n_genres=n_genres,\n",
    "            dim=dim,\n",
    "            learning_rate=learning_rate,\n",
    "            modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model.graph) as sess:\n",
    "    model.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Data Function For Predict: user_item_data\n",
    "1. For fast prediction, make another function\n",
    "2. 原始的dataFn是 user meta + item meta, 若預測671 users對上9125 items的評分需要671 x 9125筆資料, serving time會太久\n",
    "3. 現在預測所有users對所有items的資料只需要671 + 9125筆資料, 執行graph中的pred節點即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for u_data, items in user_item_data(trProcessed, np.arange(0, 5), movie_trans, n_batch=5):\n",
    "    break\n",
    "pd.DataFrame(u_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(items).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 單一user rating分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred = model.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    mae_ = model.evaluateMAE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "    rmse_ = model.evaluateRMSE(sess, dataFn(teProcessed, n_batch=n_batch, shuffle=False))\n",
    "\n",
    "print()\n",
    "print(\"MAE loss: \", mae_)\n",
    "print(\"RMSE loss: \", rmse_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 22\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# .query(\"rating != 0\")\n",
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model.predict(sess, u_data, items))\n",
    "        # predMat.append(model.predict(sess, u_data))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos_ary, neg_ary = [], []\n",
    "for label in teRatingMat:\n",
    "    label = label[label != 0]\n",
    "    pos_ary.append(sum(label >= 4))\n",
    "    neg_ary.append(sum(label < 4))\n",
    "    # print(\"pos: {}, neg: {}\".format(sum(label >= 4), sum(label < 4)))\n",
    "    \n",
    "def draw_pos_neg(idx):\n",
    "    pd.DataFrame(\n",
    "        index=idx,\n",
    "        data={\"pos\": np.array(pos_ary)[idx], \"neg\": np.array(neg_ary)[idx]}).plot.bar(figsize=(10, 5), alpha=0.8)\n",
    "    plt.show()\n",
    "\n",
    "# 可調整index觀察各user rating分布 ex: [0:10] [10:20] [600:610]\n",
    "draw_pos_neg(np.arange(len(teRatingMat))[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Test Data Movie Ratings(觀察上圖)\n",
    "1. 0號, 2號, 5號, 9號 user 正向評價數量 < 10, 就算model全部預測命中, 命中率也不會是 100%!\n",
    "    ex: 0號user只有1個正向評價, 全部命中也只得到0.1的分數\n",
    "2. 3號user正向評價是負向評價的5倍多, 就算亂猜, 中的機率也很高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sum(np.sum(teRatingMat >= 4, 1) < 10)\n",
    "print(\"{} 個user正向評價總數小於10!\".format(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"rating數量 >= 10 且 負評價數量 >= 正評價數量 有 [{}] 人\".format(sum(strict_condition(label) for label in teRatingMat)))\n",
    "print(\"rating正評價數量 >= 0 且 rating負評價數量 >= 0 有 [{}] 人\".format(sum(norm_condition(label) for label in teRatingMat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision At K: \n",
    "> **預測分數高(rating >= 4)的前10部電影, 和實際user rating比較, 觀察命中率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG: Normalized Discounted Cumulative Gain\n",
    "1. A measure of ranking quality.\n",
    "2. loop 每一位user, prediciton score排序後計算NDCG\n",
    "    <br/>$$ DCG_p = \\sum^p_{i = 1} \\frac{2^{rel_i} - 1}{log_2(i + 1)} $$<br/>\n",
    "3. IDCG: Ideal DCG, 為理想狀態下的DCG分數, 即model全部命中的DCG分數, 而NDCG: Normalized DCG, 公式如下\n",
    "    <br/>$$ NDCG_p = \\sum^p_{i = 1} \\frac{DCG_p}{IDCG_p} $$<br/>\n",
    "4. 所以NDCG是一個比值, 介於0 ~ 1之間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## MF + DNN with Cross Entropy Loss Function (Sigmoid Cross Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在Rating只有1(正向) or 0(負向)時該如何處理?\n",
    "1. 繼承ModelMfDNN, 改動loss function, 因為使用sigmoid, rating欄位需要變成0 or 1\n",
    "2. 每一個epoch觀察cross entropy的loss之外, 也觀察RMSE loss的變化(計算RMSE維持原rating)\n",
    "3. 許多狀況很難拿到user explicit feedback, 像是movie rating, 因為沒法強迫user去rating所有movies, \n",
    "   但是應該都能夠蒐集到implicit feedback\n",
    "   ex: youtube影片觀看時間長短, 點擊率, 通常implicit feedback只能歸納為到 [0, 1] 之間, 0代表negative, 1代表positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMfDNNCrossEntropy(ModelMfDNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ModelMfDNNCrossEntropy, self).__init__(*args, **kwargs)\n",
    "            \n",
    "    def fit(self, sess, trainGen, testGen, reset=False, nEpoch=50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if reset:\n",
    "            print(\"reset model: clean model dir: {} ...\".format(self.modelDir))\n",
    "            self.resetModel(self.modelDir)\n",
    "        # try: 試著重上次儲存的model再次training\n",
    "        self.ckpt(sess, self.modelDir)\n",
    "\n",
    "        start = time.time()\n",
    "        print(\"%s\\t%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"XEntropy Error\", \"Val XENT Error\", \"Val RMSE Error\", \"Elapsed Time\"))\n",
    "        minLoss = 1e7\n",
    "        for ep in range(1, nEpoch + 1):\n",
    "            tr_loss, tr_total =  0, 0\n",
    "            for i, data in enumerate(trainGen(), 1):\n",
    "                loss, _ = sess.run([self.loss, self.train_op], feed_dict=self.feed_dict(data, mode=\"train\"))\n",
    "                batch_len = len(data[\"query_movie_ids\"])\n",
    "                tr_loss += loss * batch_len\n",
    "                tr_total += batch_len\n",
    "                print(\"\\rtrain loss: {:.3f}\".format(loss), end=\"\")\n",
    "                \n",
    "            if testGen is not None:\n",
    "                te_rmse_loss, te_xent_loss  = self.epochLoss(sess, testGen)\n",
    "\n",
    "            tpl = \"\\r%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\"\n",
    "            if minLoss > te_xent_loss:\n",
    "                tpl += \", saving ...\"\n",
    "                self.saver.save(sess, os.path.join(self.modelDir, 'model'), global_step=ep)\n",
    "                minLoss = te_xent_loss\n",
    "\n",
    "            end = time.time()\n",
    "            print(tpl % (ep, tr_loss / tr_total, te_xent_loss, te_rmse_loss, end - start))\n",
    "            start = end\n",
    "        return self\n",
    "\n",
    "    def epochLoss(self, sess, dataGen):\n",
    "        tot_xent_loss, tot_rmse_loss, tot_cnt = 0, 0, 0\n",
    "        for data in dataGen():\n",
    "            xent_loss, rmse_loss = sess.run([self.loss, self.rmse_loss], feed_dict=self.feed_dict(data, mode=\"eval\"))\n",
    "            batch_len = len(data[\"query_movie_ids\"])\n",
    "            tot_rmse_loss += rmse_loss ** 2 * batch_len\n",
    "            tot_xent_loss += xent_loss * batch_len\n",
    "            tot_cnt += batch_len\n",
    "        return np.sqrt(tot_rmse_loss / tot_cnt), tot_xent_loss / tot_cnt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN: Item(Movie Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_dnn(self):\n",
    "    with tf.variable_scope(\"dnn\"):\n",
    "        init_fn = tf.glorot_uniform_initializer()\n",
    "        # encode [item embedding + item metadata]\n",
    "        self.item_repr = tf.concat([self.candidate_emb, self.genres_emb, self.avg_rating[:, tf.newaxis], self.year[:, tf.newaxis]], 1)\n",
    "        self.candidate_bias = tf.matmul(self.item_repr, self.b_candidate_movie_id[:, tf.newaxis])\n",
    "        \n",
    "        # Do: 目前兩層DNN, 可以試著增加或減少hidden layer, 但是最後要跟query_emb內積, 最後一層維度必須 = dim\n",
    "        # Example:\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, 64, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, 32, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "            # self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "\n",
    "        # Do: 若有overfitting後可嘗試dropout\n",
    "        # dp_scale = 0.5\n",
    "        self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "        # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)\n",
    "        self.item_repr = tf.layers.dense(self.item_repr, dim, kernel_initializer=init_fn, activation=tf.nn.relu)\n",
    "        # self.item_repr = tf.layers.dropout(self.item_repr, dp_scale, training=self.isTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation: 帶公式\n",
    "$$ \\sigma(u_i \\cdot m_j + b_u + b_m + b_{global}) $$\n",
    "1. pred節點需要加上tf.nn.sigmoid function, **infer節點在tf.nn.sigmoid_cross_entropy_with_logits會加上sigmoid function, 不須加入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_computation(self):\n",
    "    with tf.variable_scope(\"computation\"):\n",
    "        infer = tf.reduce_sum(self.query_emb * self.item_repr, 1, keep_dims=True)\n",
    "        infer = tf.add(infer, self.b_global)\n",
    "        infer = tf.add(infer, self.query_bias)\n",
    "        self.infer = tf.add(infer, self.candidate_bias, name=\"infer\")\n",
    "\n",
    "        # one query for all items\n",
    "        self.pred = tf.matmul(self.query_emb, tf.transpose(self.item_repr)) + \\\n",
    "                    tf.reshape(self.candidate_bias, (1, -1)) + self.query_bias + self.b_global\n",
    "        self.pred = tf.nn.sigmoid(self.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss: 將原本Regression的Loss(MSE or RMSE or MAE) 改成Sigmoid + Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graph_loss(self):\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        # if rating >= 4 then 1 else 0\n",
    "        self.alter_rating = tf.to_float(self.rating >= 4)\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                labels=self.alter_rating[:, np.newaxis], logits=self.infer))\n",
    "\n",
    "        # for observation\n",
    "        self.rmse_loss = tf.sqrt(tf.losses.mean_squared_error(labels=self.alter_rating[:, tf.newaxis], predictions=self.infer))\n",
    "        self.mae_loss = tf.reduce_mean(tf.abs(self.infer - self.alter_rating[:, tf.newaxis]))\n",
    "        \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.AdagradOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss)\n",
    "        # self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "n_batch = 128\n",
    "# Do: 嘗試不同的learning_rate [0.1, 0.001, 0.0001]\n",
    "learning_rate = 0.0001\n",
    "# Do: 嘗試不同的dim [8, 16, 20, 32]\n",
    "dim = 16\n",
    "# 非必要: 改動model dir\n",
    "modelDir = \"./model/model_mf_with_dnn_xent\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model_xent = ModelMfDNNCrossEntropy(\n",
    "                n_items=nMovies,\n",
    "                n_genres=n_genres,\n",
    "                dim=dim,\n",
    "                learning_rate=learning_rate,\n",
    "                modelDir=modelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    model_xent.fit(sess, dataFn(trProcessed, n_batch=n_batch, shuffle=True), dataFn(teProcessed, n_batch=n_batch), nEpoch=10, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC_CURVE (Receiver operating characteristic), AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coord = teRatingMat.nonzero()\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    predMat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        predMat.append(model_xent.predict(sess, u_data, items))\n",
    "    predMat = np.vstack(predMat)\n",
    "# regard rating >= 4 as user like this movie\n",
    "drawRocCurve((teRatingMat[coord] >= 4).astype(int), predMat[coord])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 觀察單一user與預測分布圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# user id from 0 ~ 670\n",
    "uid = 22\n",
    "u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred = model_xent.predict(sess, u_queries, movies_meta)\n",
    "print(\"shape: \", pred.shape, minmax_scale(pred.T).T)\n",
    "\n",
    "nnzCoord = teRatingMat[uid].nonzero()\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].set_title(\"pred distribution\")\n",
    "pd.Series(pred.ravel()[nnzCoord]).hist(bins=30, ax=ax[0])\n",
    "ax[1].set_title(\"real distribution\")\n",
    "pd.Series(map(lambda e: e, teRatingMat[uid][nnzCoord])).hist(bins=30, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User導向評估(Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 可給定user id細看每個user的rating與model預測效果\n",
    "# valid user id from 0 ~ 670\n",
    "uid = 22\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    u_queries, movies_meta = list(user_item_data(trProcessed, [uid], movie_trans))[0]\n",
    "    recomm = model_xent.predict(sess, u_queries, movies_meta).ravel()\n",
    "recommDf = pd.DataFrame(data={\n",
    "              \"userId\": uid,\n",
    "              \"movieId\": range(len(recomm)), \n",
    "              \"title\": midMap[np.arange(len(recomm))].values, \n",
    "              \"rating\": teRatingMat[uid, range(len(recomm))],\n",
    "              \"predRating\": recomm},\n",
    "             columns=(\"userId\", \"movieId\", \"title\", \"rating\", \"predRating\"))\n",
    "# ascending 可以調整True or False觀察結果\n",
    "recommDf.query(\"rating != 0\").sort_values(\"rating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model導向評估(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommDf.query(\"rating != 0\").sort_values(\"predRating\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision At 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "n_batch = 128\n",
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, i_data in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=n_batch):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, i_data))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "print( \"strict condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, strict_condition, k=10) )\n",
    "print( \"norm condition precision at 10: \", precision_at_k(teRatingMat, pred_mat, norm_condition, k=10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model_xent.graph) as sess:\n",
    "    pred_mat = []\n",
    "    for u_data, items in user_item_data(teProcessed, np.arange(nUsers), movie_trans, n_batch=128):\n",
    "        pred_mat.append(model_xent.predict(sess, u_data, items))\n",
    "    pred_mat = np.vstack(pred_mat)\n",
    "    \n",
    "strict_ndcg = all_user_ndcg(teRatingMat, pred_mat, strict_condition, label_thres=4, k=10)\n",
    "norm_ndcg = all_user_ndcg(teRatingMat, pred_mat, norm_condition, label_thres=4, k=10)\n",
    "print(\"strict condition ndcg at 10: \", strict_ndcg)\n",
    "print(\"norm condition ndcg at 10: \", norm_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "## 取出movies embedding, 使用cosine similarity列出最相似的電影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies[movies.title.str.contains(\"Toy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def most_like(model, seed_movie, k=10):\n",
    "    \"\"\"給定某一部電影, 使用model裡movies embedding找尋cosine相似度高的其他電影!\"\"\"\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        model.ckpt(sess, model.modelDir)\n",
    "        user_queries, items = list(user_item_data(trProcessed, [0], movie_trans))[0]\n",
    "        movie_emb = sess.run(model.item_repr, feed_dict={\n",
    "            model.isTrain: False,\n",
    "            model.genres: items[\"genres\"],\n",
    "            model.genres_len: items[\"genres_len\"],\n",
    "            model.avg_rating: items[\"avg_rating\"],\n",
    "            model.year: items[\"year\"],\n",
    "            model.candidate_movie_id: items[\"candidate_movie_id\"]\n",
    "        })\n",
    "        \n",
    "    most_like = cosine_similarity(movie_emb[seed_movie][np.newaxis, :], movie_emb).ravel().argsort()[::-1][:k]\n",
    "    return movies.iloc[most_like]\n",
    "\n",
    "# mse訓練出來的model\n",
    "most_like(model, 0, k=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cross entropy訓練出來的model\n",
    "most_like(model_xent, 0, k=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Observations\n",
    "1. 可觀察到使用movie embedding以cosine相似度找相似的movie已經有效果, 但是我們用的並不是自己組合出來的movie features, 而是embedding, 代表movie帶入features所train出來的embedding是有效果的\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
